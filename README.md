# Table of Contents
NOTE: This repository is for learning purposes only. Please follow the Coursera honor code. I've posted the answers here with the intent that it helps with debugging your own code. I encourage you to utilize the discussion forums available via Coursera and use this repo to understand why your program isn't working as expected. Best of luck!

<p align="center">
  <img width="730" height="550" src="https://github.com/jessxphil/mathematics-of-machine-learning-multivariate-calculus/blob/master/image-ml-calc.png">
</p>

Here's a peak into what you'll learn in the course! Understanding calculus is central to understanding machine learning. You can think of calculus as simply a set of tools for analysing the relationship between functions and their inputs. Often, in machine learning, we are trying to find the inputs which enable a function to best match the data. In the image above, the Sandbox exercise will build an understanding of what the Jacobian is, and how this can be used to find the minimum of a function by clicking on any point in the sandpit, and thereby measuring the negative of the Jacobian. 

## Objectives

<b>Week 1:</b> Recall the definition of differentiation. Apply differentiation to simple functions. Describe the utility of time saving rules. Apply sum, product and chain rule. 

<b>Week 2:</b> Recognize that differentiation can be applied to multiple variables in an equation. Recognise the utility of vector/matrix structures in multivariate calculus. Examine 2D problems using the Jacobian. 

<b>Week 3:</b> Apply multivariate chain rule to differentiate nested functions. Explain structure and function of a neural net. Apply multivariate calculate tools to relate network parameters to outputs. Implement backpropagation on a small neural network. 

<b>Week 4:</b> Recognise power series approximations to functions. Interpret behaviour of power series approximations for ill-behaved functions. Explain meaning of linearization. Select appropriate representation of multivariate approximations. 

<b>Week 5:</b> Recognize the principles of gradient descent. Implement optimisation using multivariate calculus. Examine cases where the method fails to return the best solution. Solve gradient descent problems using Lagrange Multipliers. 

<b>Week 6:</b> Describe regression as minimization of errors. Distinguish appropriate from inappropriate models for particular data sets. Calculate multivariate calculus objects to perform a regression. Code a non-linear function to data using gradient descent. 


## Assignments
- The Sandpit Part I ([Solution](https://github.com/jessxphil/mathematics-of-machine-learning-multivariate-calculus/blob/master/assignment-1/the-sandpit-part-1.ipynb))
- The Sandpit Part II ([Solution](https://github.com/jessxphil/mathematics-of-machine-learning-multivariate-calculus/blob/master/assignment-1/the-sandpit-part-2.ipynb))
- Back Propogation ([Solution](https://github.com/jessxphil/mathematics-of-machine-learning-multivariate-calculus/blob/master/assignment-2/i-heart-back-propagation.ipynb))
- Fitting the Distribution of Heights Data ([Solution](https://github.com/jessxphil/mathematics-of-machine-learning-multivariate-calculus/blob/master/assignment-3/fitting-distribution-height-data.ipynb))
