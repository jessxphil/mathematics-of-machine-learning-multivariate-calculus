{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Backpropagation\n## Instructions\n\nIn this assignment, you will train a neural network to draw a curve.\nThe curve takes one input variable, the amount travelled along the curve from 0 to 1, and returns 2 outputs, the 2D coordinates of the position of points on the curve.\n\nTo help capture the complexity of the curve, we shall use two hidden layers in our network with 6 and 7 neurons respectively.\n\n![Neural network with 2 hidden layers. There is 1 nodes in the zeroth layer, 6 in the first, 7 in the second, and 2 in the third.](readonly/bigNet.png \"The structure of the network we will consider in this assignment.\")\n\nYou will be asked to complete functions that calculate the Jacobian of the cost function, with respect to the weights and biases of the network. Your code will form part of a stochastic steepest descent algorithm that will train your network.\n\n### Matrices in Python\nRecall from assignments in the previous course in this specialisation that matrices can be multiplied together in two ways.\n\nElement wise: when two matrices have the same dimensions, matrix elements in the same position in each matrix are multiplied together\nIn python this uses the \u0027$*$\u0027 operator.\n```python\nA \u003d B * C\n```\n\nMatrix multiplication: when the number of columns in the first matrix is the same as the number of rows in the second.\nIn python this uses the \u0027$@$\u0027 operator\n```python\nA \u003d B @ C\n```\n\nThis assignment will not test which ones to use where, but it will use both in the starter code presented to you.\nThere is no need to change these or worry about their specifics.\n\n### How to submit\nTo complete the assignment, edit the code in the cells below where you are told to do so.\nOnce you are finished and happy with it, press the **Submit Assignment** button at the top of this worksheet.\nTest your code using the cells at the bottom of the notebook before you submit.\n\nPlease don\u0027t change any of the function names, as these will be checked by the grading script.",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Feed forward\n\nIn the following cell, we will define functions to set up our neural network.\nNamely an activation function, $\\sigma(z)$, it\u0027s derivative, $\\sigma\u0027(z)$, a function to initialise weights and biases, and a function that calculates each activation of the network using feed-forward.\n\nRecall the feed-forward equations,\n$$ \\mathbf{a}^{(n)} \u003d \\sigma(\\mathbf{z}^{(n)}) $$\n$$ \\mathbf{z}^{(n)} \u003d \\mathbf{W}^{(n)}\\mathbf{a}^{(n-1)} + \\mathbf{b}^{(n)} $$\n\nIn this worksheet we will use the *logistic function* as our activation function, rather than the more familiar $\\tanh$.\n$$ \\sigma(\\mathbf{z}) \u003d \\frac{1}{1 + \\exp(-\\mathbf{z})} $$\n\nThere is no need to edit the following cells.\nThey do not form part of the assessment.\nYou may wish to study how it works though.\n\n**Run the following cells before continuing.**",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "%run \"readonly/BackpropModule.ipynb\"\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Populating the interactive namespace from numpy and matplotlib\nIn [4]:\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "# PACKAGE\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "# PACKAGE\n# First load the worksheet dependencies.\n# Here is the activation function and its derivative.\nsigma \u003d lambda z : 1 / (1 + np.exp(-z))\nd_sigma \u003d lambda z : np.cosh(z/2)**(-2) / 4\n\n# This function initialises the network with it\u0027s structure, it also resets any training already done.\ndef reset_network (n1 \u003d 6, n2 \u003d 7, random\u003dnp.random) :\n    global W1, W2, W3, b1, b2, b3\n    W1 \u003d random.randn(n1, 1) / 2\n    W2 \u003d random.randn(n2, n1) / 2\n    W3 \u003d random.randn(2, n2) / 2\n    b1 \u003d random.randn(n1, 1) / 2\n    b2 \u003d random.randn(n2, 1) / 2\n    b3 \u003d random.randn(2, 1) / 2\n\n# This function feeds forward each activation to the next layer. It returns all weighted sums and activations.\ndef network_function(a0) :\n    z1 \u003d W1 @ a0 + b1\n    a1 \u003d sigma(z1)\n    z2 \u003d W2 @ a1 + b2\n    a2 \u003d sigma(z2)\n    z3 \u003d W3 @ a2 + b3\n    a3 \u003d sigma(z3)\n    return a0, z1, a1, z2, a2, z3, a3\n\n# This is the cost function of a neural network with respect to a training set.\ndef cost(x, y) :\n    return np.linalg.norm(network_function(x)[-1] - y)**2 / x.size",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Backpropagation\n\nIn the next cells, you will be asked to complete functions for the Jacobian of the cost function with respect to the weights and biases.\nWe will start with layer 3, which is the easiest, and work backwards through the layers.\n\nWe\u0027ll define our Jacobians as,\n$$ \\mathbf{J}_{\\mathbf{W}^{(3)}} \u003d \\frac{\\partial C}{\\partial \\mathbf{W}^{(3)}} $$\n$$ \\mathbf{J}_{\\mathbf{b}^{(3)}} \u003d \\frac{\\partial C}{\\partial \\mathbf{b}^{(3)}} $$\netc., where $C$ is the average cost function over the training set. i.e.,\n$$ C \u003d \\frac{1}{N}\\sum_k C_k $$\nYou calculated the following in the practice quizzes,\n$$ \\frac{\\partial C}{\\partial \\mathbf{W}^{(3)}} \u003d\n   \\frac{\\partial C}{\\partial \\mathbf{a}^{(3)}}\n   \\frac{\\partial \\mathbf{a}^{(3)}}{\\partial \\mathbf{z}^{(3)}}\n   \\frac{\\partial \\mathbf{z}^{(3)}}{\\partial \\mathbf{W}^{(3)}}\n   ,$$\nfor the weight, and similarly for the bias,\n$$ \\frac{\\partial C}{\\partial \\mathbf{b}^{(3)}} \u003d\n   \\frac{\\partial C}{\\partial \\mathbf{a}^{(3)}}\n   \\frac{\\partial \\mathbf{a}^{(3)}}{\\partial \\mathbf{z}^{(3)}}\n   \\frac{\\partial \\mathbf{z}^{(3)}}{\\partial \\mathbf{b}^{(3)}}\n   .$$\nWith the partial derivatives taking the form,\n$$ \\frac{\\partial C}{\\partial \\mathbf{a}^{(3)}} \u003d 2(\\mathbf{a}^{(3)} - \\mathbf{y}) $$\n$$ \\frac{\\partial \\mathbf{a}^{(3)}}{\\partial \\mathbf{z}^{(3)}} \u003d \\sigma\u0027({z}^{(3)})$$\n$$ \\frac{\\partial \\mathbf{z}^{(3)}}{\\partial \\mathbf{W}^{(3)}} \u003d \\mathbf{a}^{(2)}$$\n$$ \\frac{\\partial \\mathbf{z}^{(3)}}{\\partial \\mathbf{b}^{(3)}} \u003d 1$$\n\nWe\u0027ll do the J_W3 ($\\mathbf{J}_{\\mathbf{W}^{(3)}}$) function for you, so you can see how it works.\nYou should then be able to adapt the J_b3 function, with help, yourself.",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "# GRADED FUNCTION\n\n# Jacobian for the third layer weights. There is no need to edit this function.\ndef J_W3 (x, y) :\n    # First get all the activations and weighted sums at each layer of the network.\n    a0, z1, a1, z2, a2, z3, a3 \u003d network_function(x)\n    # We\u0027ll use the variable J to store parts of our result as we go along, updating it in each line.\n    # Firstly, we calculate dC/da3, using the expressions above.\n    J \u003d 2 * (a3 - y)\n    # Next multiply the result we\u0027ve calculated by the derivative of sigma, evaluated at z3.\n    J \u003d J * d_sigma(z3)\n    # Then we take the dot product (along the axis that holds the training examples) with the final partial derivative,\n    # i.e. dz3/dW3 \u003d a2\n    # and divide by the number of training examples, for the average over all training examples.\n    J \u003d J @ a2.T / x.size\n    # Finally return the result out of the function.\n    return J\n\n# In this function, you will implement the jacobian for the bias.\n# As you will see from the partial derivatives, only the last partial derivative is different.\n# The first two partial derivatives are the same as previously.\n# \u003d\u003d\u003dYOU SHOULD EDIT THIS FUNCTION\u003d\u003d\u003d\ndef J_b3 (x, y) :\n    # As last time, we\u0027ll first set up the activations.\n    a0, z1, a1, z2, a2, z3, a3 \u003d network_function(x)\n    # Next you should implement the first two partial derivatives of the Jacobian.\n    # \u003d\u003d\u003dCOPY TWO LINES FROM THE PREVIOUS FUNCTION TO SET UP THE FIRST TWO JACOBIAN TERMS\u003d\u003d\u003d\n    J \u003d 2 * (a3 - y)\n    J \u003d J * d_sigma(z3)\n    # For the final line, we don\u0027t need to multiply by dz3/db3, because that is multiplying by 1.\n    # We still need to sum over all training examples however.\n    # There is no need to edit this line.\n    J \u003d np.sum(J, axis\u003d1, keepdims\u003dTrue) / x.size\n    return J",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Layer 1 is very similar to Layer 2, but with an addition partial derivative term.\n$$ \\frac{\\partial C}{\\partial \\mathbf{W}^{(1)}} \u003d\n   \\frac{\\partial C}{\\partial \\mathbf{a}^{(3)}}\n   \\left(\n   \\frac{\\partial \\mathbf{a}^{(3)}}{\\partial \\mathbf{a}^{(2)}}\n   \\frac{\\partial \\mathbf{a}^{(2)}}{\\partial \\mathbf{a}^{(1)}}\n   \\right)\n   \\frac{\\partial \\mathbf{a}^{(1)}}{\\partial \\mathbf{z}^{(1)}}\n   \\frac{\\partial \\mathbf{z}^{(1)}}{\\partial \\mathbf{W}^{(1)}}\n   ,$$\n$$ \\frac{\\partial C}{\\partial \\mathbf{b}^{(1)}} \u003d\n   \\frac{\\partial C}{\\partial \\mathbf{a}^{(3)}}\n   \\left(\n   \\frac{\\partial \\mathbf{a}^{(3)}}{\\partial \\mathbf{a}^{(2)}}\n   \\frac{\\partial \\mathbf{a}^{(2)}}{\\partial \\mathbf{a}^{(1)}}\n   \\right)\n   \\frac{\\partial \\mathbf{a}^{(1)}}{\\partial \\mathbf{z}^{(1)}}\n   \\frac{\\partial \\mathbf{z}^{(1)}}{\\partial \\mathbf{b}^{(1)}}\n   .$$\nYou should be able to adapt lines from the previous cells to complete **both** the weight and bias Jacobian.",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "stem_cell": {
      "cell_type": "raw",
      "source": "",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}